//Overview//

Snowflake is a 'database as a service' - a cloud based data warehouse (DW) aiming to take advantage of the flexibility, scalability, and availability offered by cloud platforms.

The advantages of the cloud can only be leveraged if the underlying software is designed to scale elastically over a pool of resources. In contrast, pre-cloud DWs are designed to run on small, static clusters. 

The nature of data has also changed. Previously, most data collected would come from business systems (CRM, ERP etc.) in a well-defined structure and volume. Today, a large share of data is from sources such as logs, web aps, social media, IoT etc. which are unpredictable in their volume and schema. Traditional DWs rely on fine-tuned data pipelines that rely on predictable data, and are therefore struggling with the shifting nature of data. Business have turned to big-data tools such as Hadoop, but these do not have the advantages of establish DW technology.

Snowflake is built from the ground up to tackle both of these issues. The core features of Snowflake are:

Database-as-a-service
	Removes the requirement of users to set up the system on machines. 
	Data must be landed in the cloud and aims for the data to be immediately query-able via the Snowflake UI or standard protocols such as ODBC. 
Relational
	Snowflake supports ANSI SQL and ACID transactions.
Semi-Structured
	Snowflake has built-in functions for traversing, flattening, and nesting semi-structured data.
	It supports popular formats (e.g. JSON, Avro) out of the box.
	Snowflake runs automatic schema discovery and stores data as columns, making queries over semi-structured data almost as fast as over relational data.
Elastic
	Storage and compute can be scaled dynamically without affecting availability or running queries.
Highly Available
	Snowflake tolerates full datacenter failures. No downtime during upgrades.
Durable
	Snowflake uses cloning, undrop, and cross-region backups to provide data durability.
Cost-Efficient
	Snowflake aims to be compute-efficient.
	All table data is compressed.
	Users pay only for what storage and compute resources they actually use.
Secure
	All data including temporary files and network traffic is encrypted end-to-end.
	No user data is exposed to the cloud platform.
	Implements role-based access control.


//Storage Vs. Compute//

Shared-nothing architecture is the dominant system architecture in data warehousing. In shared-nothing, each query node has its own local disks. The tables are horizontally partitioned across nodes, with each node being responsible for the rows on its local disks. Shared-nothing scales well for star-schema, as it is easy to join a small dimension table with a large fact table. As each node has the same responsibilities and runs on the same hardware, the software produced is usually elegant and easy to reason about.

However, shared-nothing tightly couples compute and storage. The homogeneous nature of shared-nothing hardware can become an issue when dealing with the real world:
Different Workloads:
	A system built for bulk loading (high I/O bandwidth, low compute) will be a poor fit for complex queries (which need low I/O bandwidth and high compute). The hardware design will have to make trade-offs based on expected usage cases. 
Membership Changes:
	If the set of nodes changes (e.g. node failure, system resizing), the table partitioning will need to be re-calculated. This can result in a large amount of data being reshuffled. As the nodes performing the reshuffling are the same ones meant to be running queries, significant performance issues can occur.
Online Upgrade:
	As the system expects homogeneous hardware and software, in-place upgrades made by sequentially upgrading parts of the system are difficult.

In an on-prem environment, these issues are tolerable as change and non-homogeneity can be made rare. However, in the cloud there is a wide variety of node types, frequent membership changes in order to dynamically scale resources, and online upgrades are preferred due to the benefit for fast development cycles. Therefore, Snowflake separates storage and compute. Compute is provided through a shared-nothing engine, and storage is provided by S3 (though any blob storage would work). The compute nodes cache table data to reduce network traffic. Rather than replicating the whole database (which may have large cold sections), the compute nodes use their local disks to hold temporary data and caches of hot data, bringing the performance close to a true shared-nothing system. This is called multi-cluster, shared-data architecture.


//Architecture//

Snowflake uses a service-oriented architecture where components communicate through a RESTful interface. The three architectural layers are:
Data Storage
	Uses Amazon S3 to store table data and query results.
	S3 does not allow data to be appended or overwritten-in-part. However, it can yield parts of files. This drives the architecture of Snowflakes table files.
	Tables are horizontally partitioned into large, immutable files which are equivalent to blocks or pages in other DB systems.
	The values of each attribute or column are group together and heavily compressed in PAX scheme.
	Each table file has a header which contains the offsets of each column within the file. Therefore, queries only need to pull the file header and from there locate the columns they need.
	S3 also stores the temp data produced by query operators once local disk space is full. Spilling the temp data prevents out-of-memory / out-of-disk errors.
	Metadata such as catalogue objects (which consists of which S3 files, statistics, locks, transaction logs, etc.) is stored in key-value as part of the Cloud Services Layer.
Virtual Warehouses (VWs)
	Handles query execution within clusters of virtual machines (EC2 instances).
	Each cluster is presented as a VW, where the individual EC2s are 'worker nodes'. The user does not interact with worker nodes, and worker nodes are not shared between VWs. A user can have multiple VWs.
	VWs are T-shirt sized from X-Small to XX-Large.
	VWs are pure compute resources, and can be created, destroyed, or resized on demand without affecting database state.
	Each VW has access to the same shared tables, without needing to copy data. Therefore, users can freely share and integrate data while maintaining private compute resources.
	Each individual query runs on one VW, maintaining performance isolation, but each VW may run more than one concurrent query.
	When a new query is submitted, each worker node created a worker process. The process never causes externally visible effects (as the table files are immutable). Any worker failures therefore can be easily contained and resolved with retries.
	Each worker node maintains a cache of table data on the local disk. The cache is a collection of file headers and individual columns of files that have been used by the node in the past. The cache lives for the duration of the worker node, and is shared with concurrent and subsequent worker processes / queries. To improve efficiency, queries operating on given columns will be assigned to worker nodes with the relevant caching. In the case of worker loss, caches are not automatically replaced, instead replaced as data for future queries is fetched.
	Some worker nodes may execute slower than others due to cloud issues (virtualisation issues, network contention etc). When a node completes scanning it own input files, it requests additional files from other nodes (file stealing). If the victim node has many files left to gather, it will transfer ownership of that file for the current query. The stealer node will then fetch that file from S3 rather than the peer to prevent additional load on stragglers.
	The execution engine used by nodes is proprietary. It is columnar, vectorized, and push-based. Columnar is more CPU efficient and has more opportunities for compression. Vectorised execution means that intermediate results do not have to be materialised, instead the data is processed as a pipeline in batches of a few thousand rows. Push-based refers to relational operators pushing their results to downstream operators, rather than waiting for them to pull the data, improving cache efficiency
	As queries are executed against immutable files, there is no need for transaction management.
Cloud Services
	A collection of services managing the virtual warehouses, queries, transactions, and all associated metadata (e.g. database schemas, access control etc, usage statistics etc.)
	Each service is long-lived and shared across multiple users, reducing unused administrative overhead.
	Each service is replicated for high availability and scalability.
	Queries are passed through the Cloud Services layer. Parsing, object resolution, access control, and plan optimisation is handled here. Snowflake attempts to keep the plan space small, increasing robustness at the cost of peak performance. Once the plan is complete, it is distributed to all worker nodes working the query. As the query executes, Cloud Services tracks state to collect performance stats and detect node failures. All query information is stored for performance analysis and auditing.
	Concurrency control is handled by the Cloud Services layer. ACID transactions are maintained using snapshot isolation, where all reads by a transaction see a consistent snapshot of the db made when the transaction started. As the S3 files are immutable, changes are made by replacing it with a different file including the changes. File additions and removals are tracked in the metadata (global key-value store). Snowflake also uses the snapshots to implement timetravel (accessing 'old' data during a retention period) and cloning.
	For pruning (not using data irrelevant to a query), Snowflake maintains the data distribution information for a given chunk (particularly max and min values). These values can be used to determine if a chunk is relevant to a query. E.g. if chunk one has 1 though 3 and chunk 2 has 3 through 5, and the query requires x >= 4, only chunk 2 is needed. This is maintained for pure-relational columns and also for auto-detected sub-columns in semi-structured data.


//Features//

Snowflake offers standard DW features, such as SQL support, ACID transactions, standard interfaces, stability, security etc. as well as some additional, rarely-seen features:
Pure SaaS experience
	Support standard database interfaces (JDBD, ODBC, Python PEP-0249) and interfaces with third party tools (e.g. Tableau, Informatica, Looker). It can also be accessed using only a web browser, increasing ease-of-use.
	Based on a service experience - no tuning knobs, no physical design, no storage grooming tasks.
Continuous Availability
	Modern data warehouses are expected to be continuously available. Snowflake uses fault tolerance and online upgrading to provide this.
	Fault tolerance means that Snowflake can handle individual and correlated node failure at all levels of the architecture. The storage, S3, is replicated across multiple data centres (Availability Zones in AWS). This extends to the metadata. In cases of node failure, other nodes can step in to pick up the activities. The cloud services layer is provided by stateless nodes over multiple AZs with a load balancer. Even a full AZ failure can be tolerated, with collateral being limited to queries failing due to the relevant nodes failing. A VW exists in a single AZ for networking reasons, so is fault tolerant for individual node failures (re-running failed queries), but will not tolerate an AZ failure. In that case, the user will need to actively provision a VW in a different AZ. Snowflake maintains a pool of standby nodes to replenish VWs with failed nodes.
	Online upgrading is possible as Snowflake is designed to allow multiple versions of the services to be deployed side-by-side, since all services are stateless and all hard state is stored in a transactional key-value store. Upgrades occur by deploying components with the new version and switching users and new queries over to the new components. When all old queries have completed, the old components and services are decommissioned. All services are upgraded once per week. 
Semi-Structured and Schema-Less Data
	Snowflake extends the standard SQL type system with three types for semi-structured data: VARIANT, ARRAY, and OBJECT.
	Values of type VARIANT can store any native SQL type as well as variable length ARRAYs of value, javascript-like OBJECTs, maps from strings to VARIANT values. Known as 'documents'.
	ARRAY and OBJECT are restrictions of the VARIANT type. 
	The three types are self-describing, compact binary serialisation with fast key-value lookup, type tests, comparing, and hashing. They can be used as join keys, grouping keys, and ordering keys.
	This allows Snowflake to be used as ELT rather than ETL, as there is no need to specify schemas or perform transformations on the load. The data can loaded from JSON, Avro, XML directly into a VARIANT column, where Snowflake will handle parsing and type inference. This is called 'schema later' and decouples schema-input from schema output, reducing business coupling when departments evolve their schema.
	If desired, transforms can be performed using the Snowflake / SQL functions. There is also support for user-defined-functions (UDFs) written in Javascript syntax and integrated with the VARIANT type.
	Post-Relational Operations - Most important operation on documents is extractions of elements. Either by field name (OBJECTs) or offset (ARRAYs). This an be done using SQL syntax or Javascript path syntax.
	A child element is a pointer inside the parent element, so no copying is required. Extraction is often followed by casting the element to a standard SQL type/
	Second most common operation is flattening (turning a nested document into multiple rows). Flattening can be recursive. The opposite process can also be performed (aggregation).
	Columnar Storage and Processing - Snowflake uses a hybrid columnar format. When semi-structured data is stored, the table file is analysed to perform automatic type inference and to determine which type paths are common. These are then removed from the document and stored separately as native relational data, along with metadata for pruning. During a scan, these columns can be reassembled into a single VARIANT column. Most queries allow the pre-extracted columns to be used. This is performed independently for each table file, which allows the process to handle schema evolution.
	Optimistic Conversion - Some datatypes, such as dates hit the database in the wrong format (e.g. string). These can either be converted at read, which is inefficient and does not provide pruning information, or can be performed at write which risks having the system misinterpret and lose information. Snowflake converts at write but preserves the original values.
	Performance - The query performance over semi-structured data with relatively stable and simple schemas is nearly on par with conventional relational data.
Time Travel 
	When write operations occur on a table, they create new versions of entire table files. The oldfiles are retained for a configurable period of time. Snowflake can therefore read previous versions of the database until the files expire. This is accessed through the SQL Syntax AT or BEFORE. The timestamps can be absolute, relative to the current time, or relative to previous statements. Examples in whitepaper.
	This also allows Snowflake to have the UNDROP keyword, quickly restoring tables, schemas, or whole databases.
	Cloning uses the CLONE keyword to create a new database that points directly to the original table files. This prevents any data copying. As changes are made to either database, the changes are moved to new table files and the databases drift apart.
	Combination of CLONE and AT allows creation of database snapshot after-the-fact.
Security
	Snowflake uses two-factor authentication, client side encrypted data import and export, secure data transfer and storage, and role based access control for DB objects. Data is always encrypted before transfer and before being written to local disk or S3. Therefore the data is secure end-to-end.
	Snowflake uses AES 256-bit encryption with a hierarchical key model based on AWS CloudHSM. Encryption keys are automatically rotated and re-encrypted. Encryption and key management require no configuration or management.
	Hierarchical key models constrain the amount of data each key protects. See image in whitepaper.
	Snowflake also constrains duration of key lifetime. Keys are rotated regularly (eg. monthly) with the old keys able to decrypt old data, but only the new keys able to encrypt new data. Less often (e.g. yearly) data encrypted with an old key is re-encrypted with a new key. When a key is no longer useful it is destroyed.
	Snowflake uses AWS CloudHSM to generate, store, and use the root keys of the key hierarchy. CloudHSM is a set of 'hardware security modules'. The root keys never leave the HSM, as all functions requiring the root keys are computed within the HSM. They also generate keys at the account and table levels.


//Related Work//

Amazon has a number of DBaaS products, notably Amazon Redshift. Redshift uses a shared-nothing architecture, and therefore runs into the issues mentioned above. Snowflake also includes better tooling for interpreting semi-structured data.
Goolge offers BigQuery. It has slight unorthodox SQL syntax, requires schemas, and is append only.
MS SQL Data Warehouse (Azure SQL DW) has a capped degree of concurrency and does not have the same level of support for semi-structured querying.
Systems such as MongoDB often only support complex queries through wrapper languages.
